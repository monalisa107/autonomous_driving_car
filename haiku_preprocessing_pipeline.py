# -*- coding: utf-8 -*-
"""Copy of Haiku_Preprocessing_Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DJrx-9oMMJoL0caiLHA51iEf8o86K0Wg

# **Import** **Statements**
"""

import csv
haikus_g = []
haikus_p = []
topics_g = []
topics_p = []
count = 0


with open("all_haikus.csv", newline='') as csv_read_file:
  reader = csv.DictReader(csv_read_file)

  for row in reader:
    haikus_g.append(row["Haiku G"])
    haikus_p.append(row["Haiku P"])
    topics_g.append(row["Topic G"])
    topics_p.append(row["Topic P"])
    count +=1

    # if count >= 5:
    #   break

print(count)

import random

# graphemes (topic_g = haiku_g)
# phonemes  <topic_p = haik_p>
# g2p       [haiku_g = haiku_p]
# p2g       {haiku_p = haiku_g}

# (encouragement = Need encouragement. / Making myself positive. / I want happiness.)
# <axn|ker|axjh|maxnt = niyd axn|ker|axjh|maxnt / mey|kaxng may|sehlf paa|zax|tihv / ay waant hhae|piy|naxsy>
# [need encouragement / making myself positive / i want happiness = niyd axn|ker|axjh|maxnt / mey|kaxng may|sehlf paa|zax|tihv / ay waant hhae|piy|naxs]  
# {niyd axn|ker|axjh|maxnt / mey|kaxng may|sehlf paa|zax|tihv / ay waant hhae|piy|naxs = need encouragement / making myself positive / i want happiness}

data = []

# text generation from topics using graphines
for t, h in zip(topics_g, haikus_g):
  line = "("+ t + " = " + h + ")"
  data.append(line)

# text generation from topics using ponemes
for t, h in zip(topics_p, haikus_p):
  line = "<"+ t + " = " + h + ">"
  data.append(line)

# translation from graphemes to phonemes and back
for g, p in zip(haikus_g, haikus_p):
  line = "[" + g + " = " + p + "]"
  data.append(line)
  line = "{" + p + " = " + g + "}"
  data.append(line)

random.shuffle(data)

for d in data[:20]:
  print(d)

"""*italicized text*# Step 2: Model Training"""

from transformers import TextDataset, DataCollatorForLanguageModeling
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
model = GPT2LMHeadModel.from_pretrained('gpt2-medium')

from torch.utils.data import Dataset, random_split
from transformers import TrainingArguments, Trainer

class HaikuDataset(Dataset):
    def __init__(self, txt_list, tokenizer, max_length):
        self.input_ids = []
        self.attn_masks = []
        self.labels = []
        packed_text = ""
        for i, txt in enumerate(txt_list):
            packed_text += txt
            # print(i, packed_text)
            
            if i%8 == 7:
                encodings_dict = tokenizer(packed_text, truncation=True,
                                          max_length=max_length, padding="max_length")
                self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
                self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))
                packed_text = ""

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]

max_length = max([len(tokenizer.encode(d)) for d in data])
print(max_length)

import transformers
import torch
import torch.nn.functional as F
from torch import nn
from torch.cuda.amp import custom_fwd, custom_bwd
from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise
from tqdm.auto import tqdm

tokenizer.pad_token = tokenizer.eos_token
dataset = HaikuDataset(data, tokenizer, max_length)

!mkdir checkpoints
!mkdir /checkpoints/output
!mkdir /checkpoints/logs

print(len(dataset))

from transformers import TrainerCallback

class SaveCallback(TrainerCallback):
  "A callback that prints a message at the beginning of training"

  def on_step_end(self, args, state, control, **kwargs):
    if state.global_step %5000 == 4999:
      file_name = "/checkpoints/output/gpt2-medium" + str(state.global_step+1).zfill(6) + ".pt"
      torch.save(model, file_name)

train_size = int(0.95 * len(dataset))
train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])
training_args = TrainingArguments(output_dir="/checkpoints/",num_train_epochs=5, logging_steps=1000,
                                  save_strategy="no", per_device_train_batch_size=2, per_device_eval_batch_size=2,
                                  warmup_steps=100, weight_decay=0.01, logging_dir="logs")
Trainer(model=model, args=training_args, train_dataset=train_dataset, callbacks=[SaveCallback],
        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),
                                                              'attention_mask': torch.stack([f[1] for f in data]),
                                                              'labels': torch.stack([f[0] for f in data])}).train()

torch.save(model, "/checkpoints/gpt2-medium.pt")



